<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Stefano Braghetto • Portfolio</title>
  <!-- Tailwind CSS via CDN -->
  <script src="https://cdn.tailwindcss.com"></script>
  <!-- AOS (Animate On Scroll) -->
  <link href="https://unpkg.com/aos@next/dist/aos.css" rel="stylesheet">
  <style>
    body { font-family: 'Inter', sans-serif; background-color: #f9fafb; }
    header { background: linear-gradient(135deg, #6e8efb, #a777e3); }
    footer { background: linear-gradient(135deg, #6e8efb, #a777e3);  }
  </style>
</head>
<body class="leading-relaxed text-gray-800">
  <!-- Header -->
  <header class="py-12 text-center text-white mb-12">
    <h1 class="text-4xl font-bold">Stefano Braghetto</h1>
    <p class="mt-4 text-xl">Staff Machine Learning Engineer | Game AI Reinforcement Learning</p>
  </header>

  <section class="container mx-auto px-4 mt-20 text-center mb-28">
    <!-- Profile photo -->
    <img
      src="images/StefanoBraghetto.jpg"
      alt="Photo of Stefano Braghetto"
      class="mx-auto w-32 h-32 rounded-full object-cover shadow-lg mb-6"
    />
  
    <h2 class="text-3xl font-bold mb-6">About Me</h2>
    <p class="text-gray-700 max-w-3xl mx-auto text-base leading-relaxed">
      I’m a Machine Learning Engineer with a strong interest in real-time applications of AI — particularly in gaming, motion synthesis, and reinforcement learning.  
      I enjoy working at the intersection of animation, simulation, and intelligent behavior.
      <br><br>
      Currently, I’m developing an experimental video game with friends and family. It’s a passion project that explores novel uses of AI in interactive storytelling — though I can’t share more details publicly just yet.
      <br><br>
      I also work at Retinize Limited, where we are developing an incredible tool for immersive virtual production. Animotive (<a href="https://www.animotive.com/" class="text-blue-500 underline" target="_blank">link</a>).
      <br><br>
      Most of the projects you see here reflect that same drive: solving hard problems in simulation, prediction, and embodiment. Whether through Unity, Blender, or low-level ML optimization, I love building systems that feel responsive and alive.
    </p>
  </section>
  

  <!-- Portfolio Grid -->
  <section id="portfolio" class="container mx-auto px-4 grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-8">
    <!-- Project Card 1 -->
    <div class="bg-white rounded-2xl shadow-lg overflow-hidden transform hover:scale-105 transition duration-300" data-aos="fade-up">
      <video
        class="w-full h-48 object-cover rounded-2xl"
        controls
        poster="images/loading-placeholder.jpg"
        preload="metadata"
      >
        <source src="videos/project1.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <div class="p-6">
        <h2 class="text-2xl font-semibold mb-2">Real Time Motion Prediction from VR Headset</h2>
        <p class="text-gray-600 text-sm">
          This is one of the projects I’m most proud of, because I put a huge amount of effort into solving it. It might be the most difficult problem I’ve ever tackled in my life.<br><br>
          What you’re watching is a project that aims to predict a user’s motion while they are holding the VR headset. The code belongs to Retinize Limited, for whom I developed this work.<br><br>
          What makes it especially interesting is that, since we wanted to commercialize it, we started by creating our own dataset using motion capture.<br><br>
          I’m very happy with these results—though they’ve actually improved even further since this video was recorded in February 2024.
        </p>
      </div>
    </div>
    <!-- Project Card 2 -->
    <div class="bg-white rounded-2xl shadow-lg overflow-hidden transform hover:scale-105 transition duration-300" data-aos="fade-up" data-aos-delay="100">
      <video class="w-full h-48 object-cover" controls>
        <source src="videos/project2.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <div class="p-6">
        <h2 class="text-2xl font-semibold mb-2">AI Implementation for Face Animation</h2>
        <p class="text-gray-600 text-sm">
          This is an implementation of the SelfTalk project (<a href="https://github.com/psyai-net/SelfTalk_release" class="text-blue-500 underline" target="_blank">link</a>) in Unity, with several modifications made to improve real-time performance (RTF).<br><br>
          SelfTalk enables facial animation to be generated from voice alone. In this version, instead of predicting vertex delta motion—as done in the original SelfTalk paper—we generate FLAME blendshapes directly from the audio.
        </p>
      </div>
    </div>
    <!-- Project Card 3 -->
    <div class="bg-white rounded-2xl shadow-lg overflow-hidden transform hover:scale-105 transition duration-300" data-aos="fade-up" data-aos-delay="200">
      <video class="w-full h-48 object-cover" controls>
        <source src="videos/Project3.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <div class="p-6">
        <h2 class="text-2xl font-semibold mb-2">AI Animation with AMP in Isaac Lab</h2>
        <p class="text-gray-600 text-sm">
          This was part of a much more ambitious project that I haven’t been able to finish yet: building an AI that can learn how to play football in a physics-simulated environment.<br><br>
          There are already impressive papers on this—especially from Google—but I wanted to give it a try myself.<br><br>
          What you’re seeing here is inference from Isaac Lab. Using AMP (<a href="https://github.com/xbpeng/DeepMimic" class="text-blue-500 underline" target="_blank">link</a>), I trained an AI to imitate an animation in a simulated environment—if the character stops applying forces, it falls.<br><br>
          I created a script (mainly in Blender) that takes any humanoid animation, retargets it to the standard character, and extracts the key features needed to generate a NumPy file that AMP can learn from.<br><br>
          In short, the goal is to build a neural network that uses reinforcement learning to learn any animation in a physically realistic environment.
        </p>
      </div>
    </div>
  </section>

  <!-- Articles Section -->
  <section id="articles" class="container mx-auto px-4 mt-16">
    <h2 class="text-3xl font-bold mb-6 text-center">Articles & Writing</h2>
    <div class="container mx-auto px-4 mt-12 text-center mb-8">
      <p class="text-gray-700">
        Here are some of the articles I’m able to share publicly:
      </p>
    </div>
    <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-2 gap-8">
      <!-- Article Card 1 -->
      <div class="bg-white rounded-2xl shadow-lg p-6 hover:shadow-xl transition duration-300" data-aos="fade-up">
        <h3 class="text-xl font-semibold mb-2">An uncommon enterpreneurship idea</h3>
        <p class="text-gray-600 text-sm mb-4">A not so common idea of how can AI could be used to encourage humans to master anything</p>
        <a href="documents/An_uncommon_enterpreneurship_idea.pdf" target="_blank" class="text-blue-500 font-medium underline">Read Article →</a>
      </div>
      <!-- Article Card 2 -->
      <div class="bg-white rounded-2xl shadow-lg p-6 hover:shadow-xl transition duration-300" data-aos="fade-up" data-aos-delay="100">
        <h3 class="text-xl font-semibold mb-2">The Next Communication Layer</h3>
        <p class="text-gray-600 text-sm mb-4">An article that explains how AI is developing a different way of communications between humans</p>
        <a href="documents/Next_Communication_Layer.pdf" target="_blank" class="text-blue-500 font-medium underline">Read Article →</a>
      </div>
      <!-- Article Card 3 -->
       <!--
      <div class="bg-white rounded-2xl shadow-lg p-6 hover:shadow-xl transition duration-300" data-aos="fade-up" data-aos-delay="200">
        <h3 class="text-xl font-semibold mb-2">Voice-Driven Facial Animation</h3>
        <p class="text-gray-600 text-sm mb-4">Techniques for generating realistic facial blendshapes from audio input using neural networks.</p>
        <a href="https://your-article-link.com/voice-facial-animation" target="_blank" class="text-blue-500 font-medium underline">Read Article →</a>
      </div>
      -->
    </div>
  </section>

  

  <!-- AOS Script -->
  <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
  <script>
    AOS.init({
      duration: 800,
      once: true
    });
  </script>
  <footer class="py-8 text-center text-white mt-20">
    <p class="text-sm">
          smbraghetto@gmail.com
    </p>
  </footer>
</body>
</html>
